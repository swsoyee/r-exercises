---
title: 'K-Means Clustering in R â€“ Exercises'
author: "sindri"
date: "13 April 2018"
output: rmarkdown::github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](https://www.r-exercises.com/wp-content/uploads/2018/04/6966285895_f9a694f2e4_z.jpg)

K-means is efficient, and perhaps, the most popular clustering method. It is a way for finding natural groups in otherwise *unlabeled* data. You specify the number of clusters you want defined and the algorithm minimizes the total within-cluster variance.  

In this exercise, we will play around with the base R inbuilt k-means function on some *labeled* data.  

Solutions are available [here](https://www.r-exercises.com/2018/04/13/11260/).  

## Exercise 1

Feed the columns with sepal measurements in the inbuilt `iris` data-set to the k-means; save the cluster vector of each observation. Use 3 centers and set the random seed to 1 before.  

```{r exercise-1, message=FALSE}
# Require dataset
d <- iris
# Check all the measurements
colnames(d)
# Get the columns with sepal measurements
col <- colnames(d)[grepl("sepal", tolower(colnames(d)))]
# Feed the columns into k-means
set.seed(1)
model <- kmeans(d[col], centers = 3)
# Save the cluster vector of each observation
cluster1 <- model$cluster
model
```

## Exercise 2  

Check the proportions of each species by cluster.  

```{r exercise-2, message=FALSE}
table1 <- table(cluster1, d$Species)
table1 / colSums(table1)
```

## Exercise 3

Make a plot with sepal length on the horizontal axis and width on the vertical axis. Find a way to visualize both the actual species and the cluster the algorithm is categorized into.  

```{r exercise-3, message=FALSE}
d$Cluster <- as.character(cluster1)

# Plotly cannot set color and symbol independently in a easy way, so change to ggplot
# library(plotly)
# plot_ly(data = d,
#         x =~ Sepal.Length,
#         y =~ Sepal.Width,
#         color =~ Species,
#         symbol =~ cluster,
#         type = "scatter",
#         mode = "markers")

library(ggplot2)
ggplot(d, aes(x = Sepal.Length, y = Sepal.Width, color = Species, shape = Cluster)) +
  geom_point() + 
  theme_bw()
```

## Exercise 4  

Repeat the clustering from step one, but include petal measurements also. Does the clustering reflect the actual species better now?  

```{r exercise-4, message=FALSE}
# Get the columns with sepal and petal measurements
col <- colnames(d)[grepl("sepal|petal", tolower(colnames(d)))]
# Feed the columns into k-means
set.seed(1)
model <- kmeans(d[col], centers = 3)
# Save the cluster vector of each observation
cluster2 <- model$cluster

table2 <- table(cluster2, d$Species)
table2 / colSums(table2)
```

## Exercise 5  

Create a new data-set identical to iris, but multiply the "Petal.Width" by 2. Are the results different now?    

```{r exercise-5, message=FALSE}
# Require dataset
d <- iris
# Multiple "Petal.Width" by 2
d$Petal.Width <- d$Petal.Width * 2
# Get the columns with sepal and petal measurements
col <- colnames(d)[grepl("sepal|petal", tolower(colnames(d)))]
# Feed the columns into k-means
set.seed(1)
model <- kmeans(d[col], centers = 3)
# Save the cluster vector of each observation
cluster3 <- model$cluster

table(cluster2, cluster3)
```

## Exercise 6

Standardize your new data-set so that each variable has a mean of 0 and a variance of 1; check once more if multiplying by two makes a difference.  

```{r exercise-6, message=FALSE}

```
